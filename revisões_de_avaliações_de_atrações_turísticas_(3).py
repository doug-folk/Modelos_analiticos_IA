# -*- coding: utf-8 -*-
"""Revisões_de_Avaliações_de_Atrações_Turísticas (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uYhjMjB5RB-6UJsqlAKYqs22oIXCTbm_

# **Contextualizção:**
Plataformas de avaliação online, como o TripAdvisor, tornaram-se ferramentas essenciais para o planejamento de viagens. As experiências compartilhadas por outros usuários, na forma de comentários e notas, influenciam diretamente a decisão de potenciais visitantes.
### **Nessa visão:**
 Este projeto aborda um problema de classificação multi-classe no domínio do Processamento de Linguagem Natural (NLP). A base de dados utilizada contém milhares de avaliações de 15 atrações turísticas brasileiras, cada uma contendo um texto de comentário e uma nota atribuída pelo usuário, variando de 1 a 5.
### **Objetivo Principal**
O objetivo central é desenvolver e avaliar modelos de Machine Learning capazes de predizer a nota de uma avaliação (de 1 a 5) baseando-se exclusivamente no texto do comentário.

### Configuração inicial do ambiente
"""

print("Instalando pacotes necessários...")
!pip install pandas numpy matplotlib seaborn nltk wordcloud scikit-learn xgboost

"""### Importação de bibliotecas

"""

print("Importando bibliotecas")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import RSLPStemmer
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.utils.class_weight import compute_class_weight

# Modelos de machine learning
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

import warnings
warnings.filterwarnings('ignore')

"""### Download de recursos do NLTK"""

print("Baixando recursos de linguagem portuguesa...")
nltk.download('stopwords')
nltk.download('rslp')
nltk.download('punkt')
nltk.download('punkt_tab')

"""### Configurações de visualização"""

print("Configurando visualizações...")
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (12, 8)
sns.set_palette("husl")

print("Ambiente configurado com sucesso!")

"""### Importações para manipulação de arquivos"""

print(" Preparando para receber o dataset...")
from google.colab import files
import io
import zipfile
import os

"""### Upload do arquivo"""

print("⬆ Por favor, faça upload do arquivo ZIP do dataset:")
uploaded = files.upload()

"""### Verificação do Upload"""

uploaded_files = list(uploaded.keys())
if uploaded_files:
    print("Arquivos recebidos:")
    for filename in uploaded_files:
        file_size = len(uploaded[filename]) / 1024 / 1024
        print(f"- {filename} ({file_size:.2f} MB)")
else:
    print("Nenhum arquivo foi uploadado")
    uploaded_files = []

"""### Criar pasta para extração"""

dataset_folder = "tripadvisor_dataset"
try:
    if not os.path.exists(dataset_folder):
        os.makedirs(dataset_folder)
        print(f" Pasta '{dataset_folder}' criada")
    else:
        print(f" Pasta '{dataset_folder}' já existe")
except Exception as e:
    print(f" Erro ao criar pasta: {e}")
    dataset_folder = "."

"""### Extração do arquivo ZIP"""

extracted_files = []
if uploaded_files:
    for filename in uploaded_files:
        if filename.endswith('.zip'):
            try:
                print(f"Extraindo {filename}...")
                with zipfile.ZipFile(io.BytesIO(uploaded[filename]), 'r') as zip_ref:
                    zip_ref.extractall(dataset_folder)
                    extracted_files = zip_ref.namelist()

                print(f"Arquivo '{filename}' extraído com sucesso!")
                break
            except zipfile.BadZipFile:
                print(f"O arquivo {filename} não é um ZIP válido")
            except Exception as e:
                print(f"Erro ao extrair {filename}: {e}")
else:
    print(" Nenhum arquivo para extrair")

"""### Listar arquivos extraídos"""

if extracted_files:
    print(f"\n Arquivos extraídos em '{dataset_folder}':")
    for file in extracted_files:
        file_path = os.path.join(dataset_folder, file)
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path) / 1024 / 1024
            print(f"  - {file} ({file_size:.2f} MB)")
        else:
            print(f"  - {file} (arquivo não encontrado)")
else:
    print("Nenhum arquivo foi extraído")

"""### Procurar arquivos CSV na pasta"""

csv_files = []
try:
    all_files = os.listdir(dataset_folder)
    csv_files = [f for f in all_files if f.endswith('.csv')]

    if csv_files:
        print(f"\n {len(csv_files)} arquivo(s) CSV encontrado(s):")
        for csv_file in csv_files:
            file_path = os.path.join(dataset_folder, csv_file)
            file_size = os.path.getsize(file_path) / 1024 / 1024
            print(f"  - {csv_file} ({file_size:.2f} MB)")
    else:
        print(" Nenhum arquivo CSV encontrado na pasta")

except Exception as e:
    print(f" Erro ao listar arquivos: {e}")

"""### Função para detectar encoding"""

def detect_encoding(file_path):
    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'windows-1252']
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                f.read(1024)
            return encoding
        except:
            continue
    return None

"""### Função para detectar separador"""

def detect_separator(file_path, encoding):
    if not encoding:
        return ','

    separators = [',', ';', '\t', '|']
    try:
        with open(file_path, 'r', encoding=encoding) as f:
            first_line = f.readline()
            second_line = f.readline()

        separator_counts = {}
        for sep in separators:
            count1 = first_line.count(sep)
            count2 = second_line.count(sep) if second_line else 0
            if count1 > 0 and (count2 == 0 or count1 == count2):
                separator_counts[sep] = count1

        if separator_counts:
            return max(separator_counts, key=separator_counts.get)
    except:
        pass
    return ','

"""### Agora vamos carregar o dataset"""

df = None
loaded_successfully = False

if csv_files:
    csv_path = os.path.join(dataset_folder, csv_files[0])
    print(f"\n Tentando carregar: {csv_files[0]}")

    # Detectar encoding e separador
    encoding = detect_encoding(csv_path)
    separator = detect_separator(csv_path, encoding) if encoding else ','

    if encoding:
        print(f"Encoding detectado: {encoding}")
    else:
        print("Encoding não detectado, usando utf-8 como padrão")
        encoding = 'utf-8'

    print(f"Separador detectado: '{separator}'")

"""### Descrição do Problema e Análise Exploratória dos Dados (EDA)**

 Nesta seção, realizamos uma Análise Exploratória dos Dados (EDA) para entender as características do nosso dataset. A primeira etapa é consolidar todos os arquivos de dados em um único DataFrame para garantir que toda a análise seja feita em cima da base completa.
 Para isso, vamos iterar sobre todos os arquivos encontrados na pasta `tripadvisor_dataset` e unificá-los.

"""

print("Iniciando o carregamento e consolidação dos datasets...")

dataframes = []

for csv_file in csv_files:
    file_path = os.path.join(dataset_folder, csv_file)
    print(f"Processando arquivo: {csv_file}")

    try:
        # Usando as funções de encoding e separador
        encoding = detect_encoding(file_path)
        separator = detect_separator(file_path, encoding)

        # Carrega o arquivo CSV em um DataFrame
        df_temp = pd.read_csv(file_path, encoding=encoding, sep=separator)

        df_temp['attraction'] = csv_file.replace('.csv', '')

        dataframes.append(df_temp)

    except Exception as e:
        print(f"  Erro ao carregar o arquivo {csv_file}: {e}")


df = pd.concat(dataframes, ignore_index=True)

print("\nTodos os arquivos foram consolidados com sucesso!")
print(f"O dataset final possui {df.shape[0]} linhas e {df.shape[1]} colunas.")

"""#Análise Exploratória e Pré-processamento de Dados
  Primeiro, vamos ver a estrutura do nosso dataset, as colunas existentes e se há dados faltando.


"""

print("Estrutura do DataFrame:")
df.info()

print("\nVerificando valores nulos:")
print(df.isnull().sum())

print("\nVisualizando as primeiras linhas:")
display(df.head())

"""## Análise da Variável Alvo (Rating)
 Vamos criar um gráfico para visualizar a quantidade de avaliações para cada nota.
"""

# Gerar o gráfico
plt.figure(figsize=(10, 5))
sns.countplot(x='nota', data=df)
plt.title('Distribuição das Notas (Ratings)')
plt.xlabel('Nota')
plt.ylabel('Quantidade de Avaliações')
plt.show()

print("\nContagem de avaliações por nota:")
print(df['nota'].value_counts().sort_index())

"""**Análise::** A grande maioria das avaliações são para a nota 5, o que confirma o forte desbalanceamento dos dados.

Vamos visualizar esta distribuição por cada .CSV garreado individualmente.
"""

print("Gerando gráficos de distribuição das notas por atração...")

for attraction in df['attraction'].unique():
    plt.figure(figsize=(10, 5))
    df_attraction = df[df['attraction'] == attraction]
    sns.countplot(x='nota', data=df_attraction)
    plt.title(f'Distribuição das Notas para {attraction}')
    plt.xlabel('Nota')
    plt.ylabel('Quantidade de Avaliações')
    plt.show()

print("\nGráficos de distribuição das notas por atração gerados!")

"""## Análise Estatística e Correlação

Vamos analisar as estatísticas descritivas das colunas numéricas e visualizar a correlação entre elas.
"""

print("\nEstatísticas descritivas:")
display(df.describe())

"""###Analise descritiva para os datasets oscilantes:

---


"""

print("Análise descritiva para o dataset hopi_hari:")

df_hopi_hari = df[df['attraction'] == 'hopi_hari'].copy()

print("\nEstatísticas descritivas para hopi_hari:")
display(df_hopi_hari.describe())

print("\nContagem de avaliações por nota para hopi_hari:")
display(df_hopi_hari['nota'].value_counts().sort_index())

plt.figure(figsize=(10, 5))
sns.countplot(x='nota', data=df_hopi_hari)
plt.title('Distribuição das Notas (Ratings) para Hopi Hari')
plt.xlabel('Nota')
plt.ylabel('Quantidade de Avaliações')
plt.show()

print("Visualizando a distribuição da coluna 'nota':")

# Histograma
plt.figure(figsize=(10, 5))
sns.histplot(df['nota'], bins=5, kde=True)
plt.title('Distribuição da Nota')
plt.xlabel('Nota')
plt.ylabel('Frequência')
plt.show()

print("Gerando gráfico de violino para a distribuição da nota...")

plt.figure(figsize=(10, 5))
sns.violinplot(x='nota', data=df, inner='quartile')
plt.title('Distribuição da Nota (Gráfico de Violino)')
plt.xlabel('Nota')
plt.ylabel('Densidade')
plt.show()

print("\nGráfico de violino gerado.")

"""###### Matriz de Correlação - Análise da Relação entre Colunas

Vamos visualizar a relação entre a nota e o tamanho do comentário (número de palavras). Como temos apenas uma coluna numérica relevante ('nota' e 'review_length'), a matriz de correlação será simples.
"""

df['review_length'] = df['comentario'].apply(lambda x: len(str(x).split()))

print("\nCalculando e visualizando a matriz de correlação:")
correlation_matrix = df[['nota', 'review_length']].corr()

plt.figure(figsize=(6, 4))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Matriz de Correlação')
plt.show()

print("Gerando gráfico de dispersão: Comprimento do Comentário vs. Nota...")

plt.figure(figsize=(10, 6))
sns.scatterplot(x='review_length', y='nota', data=df, alpha=0.5)
plt.title('Relação entre Comprimento do Comentário Limpo e Nota')
plt.xlabel('Comprimento do Comentário Limpo (Número de Palavras)')
plt.ylabel('Nota')
plt.show()

print("\nGráfico de dispersão gerado.")

"""## Visualização com Nuvem de Palavras

## Pré-processamento do Texto
Vamos criar uma função para limpar a coluna de comentários (review_text). Essa função irá:

* Converter o texto para minúsculas.
* Remover pontuações e números.
* Remover stopwords (palavras comuns como "de", "a", "o", "que").
* Aplicar stemming para reduzir as palavras à sua raiz
"""

stemmer = RSLPStemmer()
stop_words = set(stopwords.words('portuguese'))

def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    # Regex ajustada para ser menos restritiva, permitindo mais caracteres acentuados comuns em português
    text = re.sub(r'[^a-záéíóúâêôãõçàèìòùäëïöüñ\s]', '', text)
    tokens = word_tokenize(text, language='portuguese')
    # Removendo tokens vazios após a limpeza
    processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and word.strip()]
    return ' '.join(processed_tokens)

print("Iniciando a limpeza dos textos das avaliações com função de pré-processamento ajustada...")
df['review_cleaned'] = df['comentario'].apply(preprocess_text)
print("Limpeza de texto concluída!")

display(df[['comentario', 'review_cleaned']].head())

from wordcloud import WordCloud

print("Gerando Nuvens de Palavras para cada nota...")

for nota in sorted(df['nota'].unique()):
    plt.figure(figsize=(12, 6))

    text_subset = ' '.join(df[df['nota'] == nota]['review_cleaned'])

    if text_subset:
        wordcloud = WordCloud(
            background_color='white',
            width=800,
            height=500,
            max_words=100,
            contour_width=3,
            contour_color='steelblue',
            colormap='viridis'
        ).generate(text_subset)

        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(f'Palavras Mais Comuns para a Nota {int(nota)}', fontsize=20)
        plt.axis('off') # Remove os eixos x e y
        plt.show()
    else:
        print(f"Não há texto disponível para gerar a nuvem para a nota {int(nota)}.")

print("\nNuvens de palavras geradas!")

print("As 5 palavras mais comuns por nota:")

for nota in sorted(df['nota'].unique()):
    text_subset = ' '.join(df[df['nota'] == nota]['review_cleaned'])

    if text_subset:
        word_counts = pd.Series(text_subset.split()).value_counts()
        print(f"\nNota {int(nota)}:")
        display(word_counts.head().reset_index().rename(columns={'index': 'Palavra', 0: 'Frequência'}))
    else:
        print(f"Não há texto disponível para analisar a nota {int(nota)}.")

print("\nAnálise das palavras comuns por nota concluída!")

"""## Metodologia e Estratégia do Projeto
Técnicas a serem utilizadas
* Baseline: Começaremos com um modelo de Regressão Logística. Ele serve como um ponto de referência para o desempenho. Usamos class_weight='balanced' para forçar o modelo a prestar mais atenção nas notas raras (1, 2 e 3).

* Modelos Avançados: Para melhorar o desempenho do baseline, planejamos usar modelos mais robustos como Random Forest e XGBoost.

## Avaliação dos Métodos
* Métrica Principal: Acurácia Balanceada. Devido à enorme quantidade de notas "5", a acurácia normal seria enganosa. A Acurácia Balanceada mede o desempenho médio em todas as classes, dando uma nota justa para a performance do modelo.

* Análise de Erros: Usaremos a Matriz de Confusão para visualizar onde o modelo acerta e, mais importante, onde ele erra, nos ajudando a entender suas limitações.

## Principal Desafio
* Dados Desbalanceados: A maior dificuldade é que o dataset é dominado por avaliações de nota 5. Nossa estratégia para lidar com isso é usar a métrica de Acurácia Balanceada e técnicas de peso de classe nos modelos.

# Etapa 1:
Realizar a vetorização dos dados de texto usando TF-IDF, treinar e avaliar modelos de Machine Learning (Regressão Logística, Random Forest, XGBoost) para classificação de texto, e analisar os resultados.

## Vetorização com tf-idf
Objetivo:
Transformar os comentários limpos `review_cleaned` em vetores numéricos usando TF-IDF.
"""

print("Transformando os comentários limpos em vetores TF-IDF...")

tfidf_vectorizer = TfidfVectorizer(max_features=5000)

X = tfidf_vectorizer.fit_transform(df['review_cleaned'])

print("Vetorização TF-IDF concluída.")
print(f"Dimensões da matriz TF-IDF: {X.shape}")

"""## Divisão dos dados

Dividir o dataset em conjuntos de treino e teste.

"""

print("Separando features (X) e target (y)...")
y = df['nota']

print("Dividindo o dataset em conjuntos de treino e teste (80/20)...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Converter os rótulos das classes para inteiros começando de 0
y_train = y_train.astype(int) - 1
y_test = y_test.astype(int) - 1

print("Divisão e conversão de rótulos concluídas.")
print(f"Dimensões do conjunto de treino (X_train): {X_train.shape}")
print(f"Dimensões do conjunto de teste (X_test): {X_test.shape}")
print(f"Dimensões do target de treino (y_train): {y_train.shape}")
print(f"Dimensões do target de teste (y_test): {y_test.shape}")

"""##Bag of Worlds"""

from sklearn.feature_extraction.text import CountVectorizer

print("Transformando os comentários limpos em vetores Bag of Words...")

bow_vectorizer = CountVectorizer(max_features=5000)

X_train_bow = bow_vectorizer.fit_transform(df['review_cleaned'])


print("Vetorização Bag of Words concluída.")
print(f"Dimensões da matriz Bag of Words: {X_train_bow.shape}")

print("As 10 palavras mais comuns no vocabulário Bag of Words:")

word_counts_bow = pd.DataFrame({
    'Palavra': bow_vectorizer.get_feature_names_out(),
    'Frequência': X_train_bow.sum(axis=0).tolist()[0]
})

display(word_counts_bow.sort_values(by='Frequência', ascending=False).head(10))

"""### Baseline 1: Regressão Logística"""

print("Treinando o modelo de Regressão Logística (Baseline 1)...")

# Usando class_weight='balanced' para tentar mitigar o desbalanceamento das classes
log_reg_model = LogisticRegression(random_state=42, solver='liblinear', multi_class='auto', class_weight='balanced')

log_reg_model.fit(X_train, y_train)

print("Treinamento da Regressão Logística concluído.")

"""### Avaliação do Baseline 1 (Regressão Logística)"""

print("Avaliando o modelo de Regressão Logística...")

y_pred_lr = log_reg_model.predict(X_test)

balanced_acc_lr = balanced_accuracy_score(y_test, y_pred_lr)
print(f"\nAcurácia Balanceada (Regressão Logística): {balanced_acc_lr:.4f}")

print("\nRelatório de Classificação (Regressão Logística):")
print(classification_report(y_test, y_pred_lr))

print("\nMatriz de Confusão (Regressão Logística):")
cm_lr = confusion_matrix(y_test, y_pred_lr)

# Normalize a matriz de confusão para exibir porcentagens
cm_lr_normalized = cm_lr.astype('float') / cm_lr.sum(axis=1)[:, np.newaxis]

disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr_normalized, display_labels=log_reg_model.classes_)
# Use um formato para exibir as porcentagens
disp_lr.plot(cmap=plt.cm.Blues, values_format='.2f')
plt.title('Matriz de Confusão (Regressão Logística) - Normalizada')
plt.gca().xaxis.grid(False) # Remove as linhas de grade do eixo x
plt.gca().yaxis.grid(False) # Remove as linhas de grade do eixo y
plt.show()

print("\nAvaliação da Regressão Logística concluída.")

print("Instalando gensim para Word2Vec...")
!pip install gensim

import gensim.downloader as api
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
import numpy as np

print("Carregando modelo pré-treinado do Word2Vec...")
# Carregue um modelo pré-treinado do Word2Vec. 'glove-wiki-gigaword-100' é um exemplo.
# Você pode explorar outros modelos disponíveis com api.info()
try:
    word2vec_model = api.load("glove-wiki-gigaword-100")
    print("Modelo Word2Vec carregado com sucesso.")
except Exception as e:
    print(f"Erro ao carregar o modelo Word2Vec: {e}")
    print("Por favor, verifique se o nome do modelo está correto ou tente outro modelo disponível em gensim.downloader.api.info()")
    word2vec_model = None


def vectorize_text(text, model):
    if model is None:
        return np.zeros(100) # Retorna um vetor de zeros se o modelo não carregar
    words = text.split()
    # Filtra palavras que estão no vocabulário do modelo
    words = [word for word in words if word in model.key_to_index]
    if not words:
        return np.zeros(model.vector_size)
    # Calcula a média dos vetores das palavras
    return np.mean([model[word] for word in words], axis=0)

if word2vec_model:
    print("Aplicando vetorização Word2Vec aos comentários limpos...")
    df['review_vector_w2v'] = df['review_cleaned'].apply(lambda x: vectorize_text(x, word2vec_model))
    print("Vetorização Word2Vec concluída.")

    print("Separando features (X_w2v) e target (y_w2v) para Word2Vec...")
    X_w2v = np.vstack(df['review_vector_w2v'].values)
    y_w2v = df['nota']

    print("Dividindo o dataset Word2Vec em conjuntos de treino e teste (80/20)...")
    X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y_w2v, test_size=0.2, random_state=42, stratify=y_w2v)

    # Converter os rótulos das classes para inteiros começando de 0
    y_train_w2v = y_train_w2v.astype(int) - 1
    y_test_w2v = y_test_w2v.astype(int) - 1

    print("Divisão e conversão de rótulos para Word2Vec concluídas.")
    print(f"Dimensões do conjunto de treino (X_train_w2v): {X_train_w2v.shape}")
    print(f"Dimensões do conjunto de teste (X_test_w2v): {X_test_w2v.shape}")
    print(f"Dimensões do target de treino (y_train_w2v): {y_train_w2v.shape}")
    print(f"Dimensões do target de teste (y_test_w2v): {y_test_w2v.shape}")
else:
    print("Word2Vec model not loaded. Skipping Word2Vec vectorization and split.")

"""### Regressão Logística com Word2Vec"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt

print("Treinando Regressão Logística com Word2Vec...")

# Definindo um grid de parâmetros reduzido para acelerar a busca
param_grid_lr_w2v = {
    'C': [0.1, 1, 10],  # Reduzindo as opções para força de regularização
    'penalty': ['l2'], # Mantendo apenas l2 por compatibilidade e eficiência
    'solver': ['liblinear', 'saga'], # Algoritmo para otimização
    'max_iter': [1000] # Reduzindo o número de iterações máximas
}

# Inicializando o modelo de Regressão Logística com class_weight='balanced'
w2v_logreg_tuned = LogisticRegression(random_state=42, multi_class='auto', class_weight='balanced')

# Configurando o GridSearchCV com um número menor de folds para validação cruzada
grid_search_lr_w2v = GridSearchCV(estimator=w2v_logreg_tuned, param_grid=param_grid_lr_w2v,
                              scoring='balanced_accuracy', cv=2, n_jobs=-1, verbose=2) # Reduzindo cv para 2

# Executando a busca
grid_search_lr_w2v.fit(X_train_w2v, y_train_w2v)

print("\nOtimização de hiperparâmetros para Regressão Logística com Word2Vec concluída!")
print(f"Melhores parâmetros encontrados: {grid_search_lr_w2v.best_params_}")
print(f"Melhor acurácia balanceada no treino (com cross-validation): {grid_search_lr_w2v.best_score_:.4f}")


y_pred_w2v_lr = grid_search_lr_w2v.best_estimator_.predict(X_test_w2v)

print(f"Acurácia Balanceada (Word2Vec + Regressão Logística Otimizada): {balanced_accuracy_score(y_test_w2v, y_pred_w2v_lr):.4f}")
print("\nRelatório de Classificação (Word2Vec + Regressão Logística Otimizada):")
print(classification_report(y_test_w2v, y_pred_w2v_lr))

# Matriz de confusão normalizada (%)
cm = confusion_matrix(y_test_w2v, y_pred_w2v_lr, normalize='true') * 100
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=grid_search_lr_w2v.best_estimator_.classes_)
fig, ax = plt.subplots(figsize=(8,6))
disp.plot(cmap=plt.cm.Blues, values_format=".1f", ax=ax)
ax.grid(False)
plt.title('Matriz de Confusão Normalizada (%) — Word2Vec + Regressão Logística Otimizada')
plt.show()

"""### Vetorização com Word2Vec

Objetivo:
Transformar os comentários limpos `review_cleaned` em vetores numéricos usando Word2Vec.

Baseline 2: Naive Bayes Multinomial
"""

print("Treinando o modelo Naive Bayes Multinomial (Baseline 2)...")

mnb_model = MultinomialNB()

mnb_model.fit(X_train, y_train)

print("Treinamento do Naive Bayes Multinomial concluído.")

"""### Avaliação do Baseline 2 (Naive Bayes Multinomial)"""

print("Avaliando o modelo Naive Bayes Multinomial...")

y_pred_mnb = mnb_model.predict(X_test)

balanced_acc_mnb = balanced_accuracy_score(y_test, y_pred_mnb)
print(f"\nAcurácia Balanceada (Naive Bayes Multinomial): {balanced_acc_mnb:.4f}")

print("\nRelatório de Classificação (Naive Bayes Multinomial):")
print(classification_report(y_test, y_pred_mnb))

print("\nMatriz de Confusão (Naive Bayes Multinomial):")
cm_mnb = confusion_matrix(y_test, y_pred_mnb)

# Normalize a matriz de confusão para exibir porcentagens
cm_mnb_normalized = cm_mnb.astype('float') / cm_mnb.sum(axis=1)[:, np.newaxis]

disp_mnb = ConfusionMatrixDisplay(confusion_matrix=cm_mnb_normalized, display_labels=mnb_model.classes_)
# Use um formato para exibir as porcentagens
disp_mnb.plot(cmap=plt.cm.Blues, values_format='.2f')
plt.title('Matriz de Confusão (Naive Bayes Multinomial) - Normalizada')
plt.gca().xaxis.grid(False) # Remove as linhas de grade do eixo x
plt.gca().yaxis.grid(False) # Remove as linhas de grade do eixo y
plt.show()

print("\nAvaliação do Naive Bayes Multinomial concluída.")

"""### Comparativo dos Baselines"""

print("Comparativo dos modelos Baseline:")
print(f"- Regressão Logística (Baseline 1) Acurácia Balanceada: {balanced_acc_lr:.4f}")
print(f"- Naive Bayes Multinomial (Baseline 2) Acurácia Balanceada: {balanced_acc_mnb:.4f}")

if balanced_acc_lr > balanced_acc_mnb:
    print("\nCom base na Acurácia Balanceada, a Regressão Logística apresentou um desempenho inicial superior.")
elif balanced_acc_mnb > balanced_acc_lr:
    print("\nCom base na Acurácia Balanceada, o Naive Bayes Multinomial apresentou um desempenho inicial superior.")
else:
    print("\nOs dois modelos apresentaram Acurácia Balanceada similar.")

"""### Metodologia de escolha:

A métrica principal utilizada para comparação é a **Acurácia Balanceada**. Esta métrica é crucial devido ao desbalanceamento significativo das classes (notas de 1 a 5), onde a maioria das avaliações tem nota 5. A acurácia balanceada calcula a média da acurácia para cada classe individualmente, garantindo que o desempenho em classes minoritárias (notas 1, 2, 3 e 4) não seja mascarado pelo alto desempenho na classe majoritária (nota 5). Um modelo com maior acurácia balanceada indica que ele tem uma capacidade de predição mais equilibrada em todas as notas.

Além da Acurácia Balanceada, a **Matriz de Confusão** é utilizada para uma análise mais detalhada dos erros de cada modelo. A matriz de confusão nos permite visualizar:

*   Verdadeiros Positivos (True Positives)
*   Verdadeiros Negativos (True Negatives)
*   Falsos Positivos (False Positives)
*   Falsos Negativos (False Negatives)

Ao comparar as matrizes de confusão da Regressão Logística e do Naive Bayes Multinomial, podemos identificar onde cada modelo está acertando e, mais importante, onde está errando para cada nota. Isso nos ajuda a entender as forças e fraquezas de cada baseline. Por exemplo, um modelo pode ter uma acurácia geral maior, mas a matriz de confusão pode revelar que ele está tendo dificuldades significativas em identificar as notas mais baixas (1 e 2), o que a acurácia balanceada já sinalizaria.

**Comparando os resultados:**

 O modelo com a Acurácia Balanceada mais alta é considerado o baseline de melhor desempenho inicial para este problema, com base nos critérios e na natureza desbalanceada dos dados. A análise das matrizes de confusão complementa essa decisão.

Com base nos resultados obtidos:

*   **Regressão Logística (Baseline 1):** Acurácia Balanceada =  0.50 ou seja 50 %
*   **Naive Bayes Multinomial (Baseline 2):** Acurácia Balanceada =  0.35 ou seja 35%

Comparando os valores acima, podemos determinar qual modelo apresentou o melhor desempenho inicial equilibrado em todas as classes.

## Etapa 2: Explorando Modelos Avançados

Com os baselines definidos, vamos treinar e avaliar modelos de Machine Learning mais avançados para tentar melhorar a performance na classificação das notas.

### Modelos Selecionados:

1.  **Support Vector Machine (SVM)**: Um modelo poderoso para classificação, especialmente em dados de alta dimensão como os gerados pelo TF-IDF.
2.  **XGBoost**: um algoritmo de gradient boosting otimizado, baseado em árvores de decisão, projetado para alta performance e precisão em tarefas de classificação e regressão.
3.  **Multi-layer Perceptron (MLP)**: Uma rede neural simples que pode aprender padrões complexos nos dados de texto vetorizados.

### Modelo 1: Support Vector Machine (SVM)
"""

from sklearn.svm import LinearSVC

print("Treinando o modelo LinearSVC (SVM)...")

# Usando class_weight='balanced' para lidar com o desbalanceamento
svm_model = LinearSVC(random_state=42, dual=False, class_weight='balanced')

svm_model.fit(X_train, y_train)

print("Treinamento do LinearSVC (SVM) concluído.")

"""### Avaliação do Modelo 1 (SVM)"""

print("Avaliando o modelo LinearSVC (SVM)...")

y_pred_svm = svm_model.predict(X_test)

balanced_acc_svm = balanced_accuracy_score(y_test, y_pred_svm)
print(f"\nAcurácia Balanceada (LinearSVC - SVM): {balanced_acc_svm:.4f}")

print("\nRelatório de Classificação (LinearSVC - SVM):")
print(classification_report(y_test, y_pred_svm))

print("\nMatriz de Confusão (LinearSVC - SVM):")
cm_svm = confusion_matrix(y_test, y_pred_svm)

# Normalize a matriz de confusão para exibir porcentagens
cm_svm_normalized = cm_svm.astype('float') / cm_svm.sum(axis=1)[:, np.newaxis]

disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm_normalized, display_labels=svm_model.classes_)
# Use um formato para exibir as porcentagens
disp_svm.plot(cmap=plt.cm.Blues, values_format='.2f')
plt.title('Matriz de Confusão (LinearSVC - SVM) - Normalizada')
plt.gca().xaxis.grid(False) # Remove as linhas de grade do eixo x
plt.gca().yaxis.grid(False) # Remove as linhas de grade do eixo y
plt.show()

print("\nAvaliação do LinearSVC (SVM) concluída.")

"""### SVM com Word2Vec"""

from sklearn.svm import LinearSVC

print("Treinando SVM com Word2Vec...")

svm_w2v = LinearSVC(class_weight='balanced', max_iter=5000)
svm_w2v.fit(X_train_w2v, y_train_w2v)

y_pred_w2v_svm = svm_w2v.predict(X_test_w2v)

print(f"Acurácia Balanceada (Word2Vec + SVM): {balanced_accuracy_score(y_test_w2v, y_pred_w2v_svm):.4f}")
print("\nRelatório de Classificação (Word2Vec + SVM):")
print(classification_report(y_test_w2v, y_pred_w2v_svm))

cm = confusion_matrix(y_test_w2v, y_pred_w2v_svm, normalize='true') * 100
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm_w2v.classes_)
fig, ax = plt.subplots(figsize=(8,6))
disp.plot(cmap=plt.cm.Blues, values_format=".1f", ax=ax)
ax.grid(False)
plt.title('Matriz de Confusão Normalizada (%) — Word2Vec + SVM')
plt.show()

"""### Modelo 2: Multi-layer Perceptron (MLP)

"""

from sklearn.neural_network import MLPClassifier

print("Treinando o modelo MLPClassifier (Rede Neural)...")

mlp_model = MLPClassifier(random_state=42, max_iter=1000)

mlp_model.fit(X_train, y_train)

print("Treinamento do MLPClassifier (Rede Neural) concluído.")

"""### Avaliação do Modelo 2 (MLP)"""

print("Avaliando o modelo MLPClassifier (Rede Neural)...")

y_pred_mlp = mlp_model.predict(X_test)

balanced_acc_mlp = balanced_accuracy_score(y_test, y_pred_mlp)
print(f"\nAcurácia Balanceada (MLPClassifier - Rede Neural): {balanced_acc_mlp:.4f}")

print("\nRelatório de Classificação (MLPClassifier - Rede Neural):")
print(classification_report(y_test, y_pred_mlp))

print("\nMatriz de Confusão (MLPClassifier - Rede Neural):")
cm_mlp = confusion_matrix(y_test, y_pred_mlp)

# Normalize a matriz de confusão para exibir porcentagens
cm_mlp_normalized = cm_mlp.astype('float') / cm_mlp.sum(axis=1)[:, np.newaxis]

disp_mlp = ConfusionMatrixDisplay(confusion_matrix=cm_mlp_normalized, display_labels=mlp_model.classes_)
# Use um formato para exibir as porcentagens
disp_mlp.plot(cmap=plt.cm.Blues, values_format='.2f')
plt.title('Matriz de Confusão (MLPClassifier - Rede Neural) - Normalizada')
plt.gca().xaxis.grid(False) # Remove as linhas de grade do eixo x
plt.gca().yaxis.grid(False) # Remove as linhas de grade do eixo y
plt.show()

print("\nAvaliação do MLPClassifier (Rede Neural) concluída.")

"""### MLP (Rede Neural) com Word2Vec"""

from sklearn.neural_network import MLPClassifier

print("Treinando MLP com Word2Vec...")

mlp_w2v = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)
mlp_w2v.fit(X_train_w2v, y_train_w2v)

y_pred_w2v_mlp = mlp_w2v.predict(X_test_w2v)

print(f"Acurácia Balanceada (Word2Vec + MLP): {balanced_accuracy_score(y_test_w2v, y_pred_w2v_mlp):.4f}")
print("\nRelatório de Classificação (Word2Vec + MLP):")
print(classification_report(y_test_w2v, y_pred_w2v_mlp))

cm = confusion_matrix(y_test_w2v, y_pred_w2v_mlp, normalize='true') * 100
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mlp_w2v.classes_)
fig, ax = plt.subplots(figsize=(8,6))
disp.plot(cmap=plt.cm.Blues, values_format=".1f", ax=ax)
ax.grid(False)
plt.title('Matriz de Confusão Normalizada (%) — Word2Vec + MLP')
plt.show()

"""### Modelo 3: XGBoost

"""

from xgboost import XGBClassifier
from sklearn.utils import compute_class_weight
import numpy as np

print("Treinando o modelo XGBoost...")

# Usando scale_pos_weight para lidar com o desbalanceamento das classes
# Calcula o peso de cada classe
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
lgb_class_weight = {i: class_weights[i] for i in range(len(class_weights))}

xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss', scale_pos_weight=lgb_class_weight)

xgb_model.fit(X_train, y_train)

print("Treinamento do XGBoost concluído.")

"""###Avaliacao do modelo 3 XGBoost"""

from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

print("Avaliando o modelo XGBoost...")

y_pred_xgb = xgb_model.predict(X_test)

balanced_acc_xgb = balanced_accuracy_score(y_test, y_pred_xgb)
print(f"\nAcurácia Balanceada (XGBoost): {balanced_acc_xgb:.4f}")

print("\nRelatório de Classificação (XGBoost):")
print(classification_report(y_test, y_pred_xgb))

print("\nMatriz de Confusão (XGBoost):")
cm_xgb = confusion_matrix(y_test, y_pred_xgb)

# Normalize a matriz de confusão para exibir porcentagens
cm_xgb_normalized = cm_xgb.astype('float') / cm_xgb.sum(axis=1)[:, np.newaxis]

disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb_normalized, display_labels=xgb_model.classes_)
# Use um formato para exibir as porcentagens
disp_xgb.plot(cmap=plt.cm.Blues, values_format='.2f')
plt.title('Matriz de Confusão (XGBoost) - Normalizada')
plt.gca().xaxis.grid(False) # Remove as linhas de grade do eixo x
plt.gca().yaxis.grid(False) # Remove as linhas de grade do eixo y
plt.show()

print("\nAvaliação do XGBoost concluída.")

